{
  "design_doc": "docs/designs/DESIGN-local-llm-runtime.md",
  "branch": "impl/local-llm-runtime-v2",
  "pr_number": 1810,
  "integrity_hash": "sha256:8d8d7e89b2fc35712eb9e7a27ddc0d669965d0863d08d096574608803dd7856e",
  "issues": [
    {
      "number": 1628,
      "title": "feat(llm): complete local provider skeleton with lifecycle management",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1629,
      "title": "feat(llm): implement addon download and verification",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1630,
      "title": "feat(llm): add configurable idle timeout",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1631,
      "title": "feat(llm): implement SIGTERM handler for graceful shutdown",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1632,
      "title": "feat(llm): integrate LocalProvider into factory with config",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628,
        1630
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1633,
      "title": "ci(llm): add Rust addon build pipeline",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1634,
      "title": "test(llm): add lifecycle integration tests",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628,
        1630,
        1631
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1635,
      "title": "feat(llm): implement hardware detection",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1628
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1636,
      "title": "feat(llm): implement model selection based on hardware",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1635
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1637,
      "title": "feat(llm): implement model download with progress",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1636
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1638,
      "title": "feat(llm): integrate llama.cpp for inference",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1633,
        1637
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1672,
      "title": "feat(llm): configure model manifest with HuggingFace URLs",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1637,
        1638
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1675,
      "title": "fix(llm): daemon socket not cleaned up on SIGTERM",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1631
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1676,
      "title": "fix(llm): tokenization fails with negative count from llama_tokenize",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1638
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1639,
      "title": "feat(llm): implement GBNF grammar constraints for JSON",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1638
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1640,
      "title": "feat(llm): wire Complete RPC to llama.cpp inference",
      "status": "completed",
      "ci_status": "passed",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1639,
        1672,
        1675,
        1676
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": []
    },
    {
      "number": 1641,
      "title": "test(llm): add quality benchmark suite",
      "status": "scrutinized",
      "ci_status": "pending",
      "ci_fix_attempts": 0,
      "issue_type": "code",
      "dependencies": [
        1640
      ],
      "commits": [
        "23b96e651d2d48c250482ef12d4b271d15d846ca"
      ],
      "agent_type": "coder",
      "summary": {
        "narrative": "",
        "files_changed": [
          "internal/builders/llm_integration_test.go",
          "internal/builders/baseline_test.go"
        ],
        "tests_added": 13,
        "key_decisions": "Extended existing TestLLMGroundTruth rather than creating new test files. Used BuildResult.RepairAttempts for repair tracking. Metrics logged in test output, not separate JSON.",
        "requirements_map": [
          {
            "ac": "latency metrics",
            "status": "implemented"
          },
          {
            "ac": "repair turn tracking",
            "status": "implemented"
          },
          {
            "ac": "configurable timeout",
            "status": "implemented"
          }
        ]
      },
      "testable_scenarios": [
        "scenario-1",
        "scenario-2",
        "scenario-3",
        "scenario-4"
      ]
    },
    {
      "number": 1642,
      "title": "feat(llm): add download permission prompts and progress UX",
      "status": "pending",
      "ci_status": "pending",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1629,
        1637
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": [
        "scenario-5",
        "scenario-6",
        "scenario-7",
        "scenario-8"
      ]
    },
    {
      "number": 1643,
      "title": "feat(llm): implement tsuku llm download command",
      "status": "pending",
      "ci_status": "pending",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1629,
        1637
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": [
        "scenario-9",
        "scenario-10",
        "scenario-11",
        "scenario-12"
      ]
    },
    {
      "number": 1644,
      "title": "test(llm): add end-to-end integration test without cloud keys",
      "status": "pending",
      "ci_status": "pending",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1640,
        1632
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": [
        "scenario-13",
        "scenario-14"
      ]
    },
    {
      "number": 1645,
      "title": "docs(llm): update documentation for local inference",
      "status": "pending",
      "ci_status": "pending",
      "ci_fix_attempts": 0,
      "issue_type": "",
      "dependencies": [
        1632,
        1636
      ],
      "commits": [],
      "agent_type": "",
      "summary": null,
      "testable_scenarios": [
        "scenario-15",
        "scenario-16"
      ]
    }
  ],
  "skipped_issues": null,
  "test_plan_file": "wip/implement-doc_local-llm-runtime_test_plan.md"
}
